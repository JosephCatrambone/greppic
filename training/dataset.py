import os
import random
import string
from glob import glob, iglob

import numpy
import torch
import torchvision.transforms as transforms
from PIL import Image, ImageDraw, ImageFont
from torch.utils.data import Dataset

class TextDetectionDataset(Dataset):
	def __init__(self, base_image_folder, font_directory, transform=None, target_width: int = 128, target_height: int = 128):
		super(TextDetectionDataset, self).__init__()
		self.dir = base_image_folder
		self.resize_op = transforms.Resize(size=[target_height, target_width])
		self.target_width = target_width
		self.target_height = target_height
		self.font_choices = None
		self.font_directory = font_directory
		self.text_noise = 0.1

		#self.transform = transform
		if transform is not None:
			print("Sorry, transforms are not supported for this dataset.  We randomize inside the generator.")

		self.images = sorted(os.listdir(base_image_folder))
		self.image_center = (self.target_width//2, self.target_height//2)
		self.max_text_offset = (self.target_width//4, self.target_height//4)

	# DEBUG METHOD!
	def make_validation_set(self, max_images=100):
		"""Return a pair of images, one with the text and one with the mask, as generated by MSCOCO.
		If max_images is less than one, will return all images and masks."""
		# Our model transform does the swap to channels first, so we don't need to fret.
		images = list()
		masks = list()
		# Load the mask data.
		import json
		with open(os.path.join("datasets", "text_images_mscoco_2014", "image_data_by_name.json"), 'rt') as fin:
			img_data = json.load(fin)
		all_image_filenames = glob("datasets/text_images_mscoco_2014/all_legible_text/*")
		for img_fullpath in [random.choice(all_image_filenames) for _ in range(max_images)]:
			img_filename = os.path.split(img_fullpath)[-1]
			if img_filename not in img_data:
				print(img_filename)
				continue
			sample_annotation = """{
				'mask': [197.5, 108.0, 196.5, 118.0, 241.5, 120.2, 241.9, 109.6],
			    'class': 'machine printed',
			    'bbox': [196.5, 108.0, 45.4, 12.2],
			    'image_id': 390310,
			    'id': 117115,
			    'language': 'english',
			    'area': 461.74,
			    'utf8_string': 'BARNES',
			    'legibility': 'legible'}]}
			"""
			img = Image.open(img_fullpath).convert('RGB')
			if img.size[0] < self.target_width or img.size[1] < self.target_height:
				continue
			# Make the mask image the size of the original so the annotations match up, then we can crop later.
			mask = Image.new('L', img.size, color=0)
			d = ImageDraw.Draw(mask)
			mean_center_x = 0
			mean_center_y = 0
			for annotation in img_data[img_filename]['annotations']:
				d.polygon(annotation['mask'], fill='white')
				text_bounding_box = annotation['bbox']
				bb_x, bb_y, bb_w, bb_h = text_bounding_box
				center_x = bb_x + (bb_w//2)
				center_y = bb_y + (bb_h//2)
				mean_center_x += center_x
				mean_center_y += center_y
			center_x = mean_center_x // len(img_data[img_filename]['annotations'])
			center_y = mean_center_x // len(img_data[img_filename]['annotations'])
			# Most of the images are 640x480-ish, so if we can resize to around 256x256 and then crop we will get more in frame.
			img.resize((img.size[0]//2, img.size[1]//2))
			mask.resize((mask.size[0]//2, mask.size[1]//2))
			center_x = center_x // 2  # Don't forget to move these!
			center_y = center_y // 2
			# Try and crop around the center.
			left = max(0, center_x - self.target_width//2)
			top = max(0, center_y - self.target_height//2)
			right = left + self.target_width
			bottom = top + self.target_height
			crop_box = (left, top, right, bottom)
			img_crop = img.crop(crop_box)
			mask_crop = mask.crop(crop_box)

			# Hack: Some masks are empty and we want to shuffle our dataset a bit to emphasize interesting cases.
			# We're not training on this, so it's not a problem.
			# Keep empty images with only a 10% chance.
			if numpy.asarray(mask_crop).sum() < 10 and random.randint(0, 10) > 1:
				continue

			images.append(img_crop)
			masks.append(mask_crop)
			# Might need to break early.
			if max_images > 0 and len(images) >= max_images:
				break
		return images, masks

	def __len__(self):
		return len(self.images)

	def get_random_font(self):
		"""We need to lazy load the fonts because the dataset loaders are assumed to run out of thread and ImageFont is not serializable."""
		if self.font_choices is None:
			self.font_choices = list()
			for font_filename in iglob(self.font_directory):
				for font_size in [12, 14, 16, 24, 32, 48]:
					self.font_choices.append(ImageFont.truetype(font_filename, font_size))
		return random.choice(self.font_choices)

	def random_text_image(self):
		"""Generate randomly oriented white text on a black background.  RGB image.  Can be used as a mask."""
		# TODO: This isn't a pretty thing, but it works.
		s = "".join(random.choice(string.ascii_letters + string.punctuation + " " * 4) for _ in range(50))
		text_image = Image.new("RGB", (self.target_width, self.target_height), "black")
		d = ImageDraw.Draw(text_image)
		# d.line(((0, 100), (200, 100)), "gray")
		# d.line(((100, 0), (100, 200)), "gray")
		# Note that this translates before rotating, so our offset might be a little weird.
		text_position = (
			self.image_center[0] + random.randint(-self.max_text_offset[0], self.max_text_offset[0]),
			self.image_center[1] + random.randint(-self.max_text_offset[1], self.max_text_offset[1]),
		)
		d.text(text_position, s, fill="white", anchor="mm", font=self.get_random_font())
		text_image = text_image.rotate(random.randint(0, 359))
		return text_image

	def __getitem__(self, index):
		# We assume we're starting with PIL images for everything AND that they have no text.
		# Perform updates via the transform stack.
		img_path = os.path.join(self.dir, self.images[index])
		img_pil = Image.open(img_path).convert('RGB')

		# Randomly mutate the input image.
		if random.choice([False, True]):
			img_pil = img_pil.transpose(Image.FLIP_LEFT_RIGHT)
		if random.choice([False, True]):
			img_pil = img_pil.transpose(Image.FLIP_TOP_BOTTOM)
		if random.choice([False, True]):
			img_pil = img_pil.rotate(random.randint(0,359))

		# If the image is big enough for a random crop, do it.
		if img_pil.size[0] > self.target_width and img_pil.size[1] > self.target_height:
			left = random.randint(0, img_pil.size[0]-1-self.target_width)
			top = random.randint(0, img_pil.size[1] - 1 - self.target_height)
			img_pil = img_pil.crop((left, top, left+self.target_width, top+self.target_height))
		else:
			img_pil = img_pil.resize((self.target_width, self.target_height))

		# Generate some random text:
		text_image_mask = self.random_text_image().convert('L')

		# Glorious hack to make a red mask:
		# red_channel = img_pil[0].point(lambda i: i < 100 and 255)

		# Draw the text image on top of our sample image.
		# if (red * 0.299 + green * 0.587 + blue * 0.114) > 186 use  # 000000 else use #ffffff
		total_color = [0, 0, 0]
		total_pixels = 0
		for y in range(self.target_height):
			for x in range(self.target_width):
				mask = text_image_mask.getpixel((x,y))
				if mask > 128:
					px = img_pil.getpixel((x,y))
					total_color[0] += px[0]
					total_color[1] += px[1]
					total_color[2] += px[2]
					total_pixels += 1

		# In the off chance we have a completely blank image, save some compute.
		if total_pixels > 0:
			avg_r = total_color[0]//total_pixels
			avg_g = total_color[1]//total_pixels
			avg_b = total_color[2]//total_pixels

			# Default to light color...
			text_color = [random.randint(200, 255), random.randint(200, 255), random.randint(200, 255)]
			if avg_r*0.299 + avg_g*0.587 + avg_b * 0.114 > 150: # 186 comes from the algorithm but is a little hard to see.
				# Unless our image is bright, in which case use dark color.
				text_color = [random.randint(0, 75), random.randint(0, 75), random.randint(0, 75)]
			# Have to convert text color to a hex string.  :rolleges:
			text_color = f"#{text_color[0]:02X}{text_color[1]:02X}{text_color[2]:02X}"
			# Make a rectangle of this color and paste it in with an image mask.
			text_color_block = Image.new("RGB", (self.target_width, self.target_height), color=text_color)
			# Maybe add noise to the color block.
			if self.text_noise > 0:
				pass
			img_pil.paste(text_color_block, (0,0), text_image_mask)

		return torch.Tensor(numpy.asarray(img_pil) / 255.0), torch.Tensor(numpy.asarray(text_image_mask) / 255.0)
		#return img_pil, text_image_mask