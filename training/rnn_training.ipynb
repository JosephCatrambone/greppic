{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageDraw2, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  8 17:28:49 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   39C    P8    14W / 215W |    162MiB /  8192MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       993      G   /usr/lib/xorg/Xorg                 86MiB |\r\n",
      "|    0   N/A  N/A      1014      G   /usr/bin/sddm-greeter              70MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Convenience method to take a triple of three floats and an image to give an appropriately formatted output image.\n",
    "\n",
    "def sample_image_from_triplet(pil_image:Image, position_triplet, tensor_width:int):\n",
    "    \"\"\"\n",
    "    Create a PIL of size tensor_width,tensor_width from the given position triplet, x, y, zoom.\n",
    "    x, y, and zoom should all be in the 0-1 range.\n",
    "    zoom behaves opposite what one would normally expect.  At 1.0, the image width is scaled down to the tensor width.\n",
    "    That is, 1.0 zoom -> full image in tensor.\n",
    "    At 0.5 -> half of the image fits in the tensor.  0.1 -> 10% of the image goes to the tensor.\n",
    "    The x and y are similar to UV coordinates, in that they describe the offset horizontally and vertically for the focus.\n",
    "    \"\"\"\n",
    "    #Image.transform(size, method, data=None, resample=Resampling.NEAREST, fill=1, fillcolor=None)\n",
    "    #Transforms this image. This method creates a new image with the given size, and the same mode as the original, and copies data to the new image using the given transform.\n",
    "    x, y, zoom = position_triplet\n",
    "    img_width, img_height = pil_image.size\n",
    "    sample_width = img_width*zoom\n",
    "    sample_height = img_height*zoom\n",
    "    nw_sw_se_ne = [\n",
    "        x*img_width, y*img_height,\n",
    "        x*img_width, y*img_height + sample_height,\n",
    "        x*img_width + sample_width, y*img_height + sample_height,\n",
    "        x*img_width + sample_width, y*img_height\n",
    "    ]\n",
    "    sample = pil_image.transform((tensor_width, tensor_width), Image.Transform.QUAD, data=nw_sw_se_ne)\n",
    "    # From source: quadrilateral warp.  data specifies the four corners given as NW, SW, SE, and NE.\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAABSklEQVR4nO1UMbbDIAyTeLn/4f59anUwYGOStuMfEFllZEmEfy97WXz2sgqZyY9MHZAAAcLFpgY1sUENUEMBDTSIRhMVE5yPq6mJbXCf+DTROBSgDxAwFXzj05QU+AQf0PniEx8CjUYZJZkwNxBcAbqEBz7NjyiT6PdPAbh+4aMbsG4geArDfm58CKBBmAbYvH+kiOvRvmwgZXOBJEBzhW/8JYJpYffgF34skEKIFJ74kFswOsS9RuHBfQDeAY0S7DXS8OCBnw2IGocADQ8+8jt9KcFM0T34ws/0miKuT/xuYESw1ChMrFB5hDmCKkA3AzzA8QijQ4atRrpTsBvQF2DaIJlYB1T+GsFSo1sTEz866H8Sb3ERUAfkANYOMYWQBJQBwU8G7CVIKa4pbPwZQX6ISUBJofNngCmCeIglxYODg4ODg4ODg3+JN69dkypCZKq2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing:\n",
    "display(Image.radial_gradient('L'))\n",
    "display(sample_image_from_triplet(Image.radial_gradient('L'), (0.0, 0.8, 0.5), 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BaseTextDataset(Dataset):\n",
    "    def __init__(self, target_width: int = 128, target_height: int = 128):\n",
    "        super(BaseTextDataset, self).__init__()\n",
    "        self.resize_op = transforms.Resize(size=[target_height, target_width])\n",
    "        self.target_width = target_width\n",
    "        self.target_height = target_height\n",
    "        self.font_choices = None\n",
    "        self.font_directory = \"fonts/*.*tf\"\n",
    "        self.font_sizes = [12, 14, 16, 24, 32, 48, 72]\n",
    "        self.text_noise = 0.1\n",
    "        self.downsample_image_before_crop = False\n",
    "        self.random_text_max_translation = 0\n",
    "        self.random_text_rotation = True\n",
    "        self.random_text_length = 17\n",
    "        self.random_text_mask_dilation = 5\n",
    "\n",
    "        with open(os.path.join(\"text_images_mscoco_2014\", \"image_data_by_name.json\"), 'rt') as fin:\n",
    "            self.img_mask_data = json.load(fin)\n",
    "\n",
    "        # Listdir will give us the filenames, but we want the full paths for loading.\n",
    "        #self.textless_images = sorted(os.listdir(os.path.join(\"text_images_mscoco_2014\", \"no_text\")))\n",
    "        self.textless_images = glob(os.path.join(\"text_images_mscoco_2014\", \"no_text\", \"*\"))\n",
    "        self.mintext_images = glob(os.path.join(\"text_images_mscoco_2014\", \"no_legible_text\", \"*\"))\n",
    "        self.sometext_images = glob(os.path.join(\"text_images_mscoco_2014\", \"some_legible_text\", \"*\"))\n",
    "        self.alltext_images = glob(os.path.join(\"text_images_mscoco_2014\", \"all_legible_text\", \"*\"))\n",
    "\n",
    "        self.image_center = (self.target_width//2, self.target_height//2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.textless_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self.alltext_images):\n",
    "            img, mask, _, _ = self.generate_image_mask_pair(index % len(self.textless_images))\n",
    "        else:\n",
    "            img, mask = self.get_dataset_image(index)\n",
    "        img = torch.Tensor(numpy.asarray(img) / 255.0)\n",
    "        mask = torch.Tensor(numpy.asarray(mask) / 255.0)\n",
    "        return img, mask\n",
    "\n",
    "    def make_validation_set(self, items=16):\n",
    "        validation_images = list()\n",
    "        validation_masks = list()\n",
    "        for i in range(items):\n",
    "            img, mask = self.get_dataset_image(i, validation_set=True)\n",
    "            # Can't generate images because that will trigger font loading and we need to wait for the workers to spin up.\n",
    "            #\timg, mask = self.generate_image(i)\n",
    "            validation_images.append(numpy.asarray(img)/255.0)\n",
    "            validation_masks.append(numpy.asarray(mask)/255.0)\n",
    "        validation_images = torch.Tensor(numpy.asarray(validation_images))\n",
    "        validation_masks = torch.Tensor(numpy.asarray(validation_masks))\n",
    "        return validation_images, validation_masks\n",
    "\n",
    "    # Pull from a real dataset.\n",
    "    def get_dataset_image(self, idx: int, validation_set: bool = False):\n",
    "        \"\"\"Return a pair of images, one with the text and one with the mask, as generated by MSCOCO.\"\"\"\n",
    "        # Our model transform does the swap to channels first, so we don't need to fret.\n",
    "        if validation_set:\n",
    "            img_fullpath = self.sometext_images[idx]\n",
    "        else:\n",
    "            img_fullpath = self.alltext_images[idx]\n",
    "        img_filename = os.path.split(img_fullpath)[-1]\n",
    "        assert img_filename in self.img_mask_data\n",
    "        sample_annotation = \"\"\"{\n",
    "            'mask': [197.5, 108.0, 196.5, 118.0, 241.5, 120.2, 241.9, 109.6],\n",
    "            'class': 'machine printed',\n",
    "            'bbox': [196.5, 108.0, 45.4, 12.2],\n",
    "            'image_id': 390310,\n",
    "            'id': 117115,\n",
    "            'language': 'english',\n",
    "            'area': 461.74,\n",
    "            'utf8_string': 'BARNES',\n",
    "            'legibility': 'legible'}]}\n",
    "        \"\"\"\n",
    "        img = Image.open(img_fullpath).convert('RGB')\n",
    "        # Make the mask image the size of the original so the annotations match up, then we can crop later.\n",
    "        mask = Image.new('L', img.size, color=0)\n",
    "        d = ImageDraw.Draw(mask)\n",
    "        mean_center_x = 0\n",
    "        mean_center_y = 0\n",
    "        # Draw all bounding polygons:\n",
    "        for annotation in self.img_mask_data[img_filename]['annotations']:\n",
    "            d.polygon(annotation['mask'], fill='white')\n",
    "            text_bounding_box = annotation['bbox']\n",
    "            bb_x, bb_y, bb_w, bb_h = text_bounding_box\n",
    "            center_x = bb_x + (bb_w//2)\n",
    "            center_y = bb_y + (bb_h//2)\n",
    "            mean_center_x += center_x\n",
    "            mean_center_y += center_y\n",
    "        center_x = mean_center_x // len(self.img_mask_data[img_filename]['annotations'])\n",
    "        center_y = mean_center_x // len(self.img_mask_data[img_filename]['annotations'])\n",
    "        # DEBUG!  Select one fo the bounding polygons to focus on at random.\n",
    "        annotation = random.choice(self.img_mask_data[img_filename]['annotations'])\n",
    "        center_x = annotation['bbox'][0] + annotation['bbox'][2]//2\n",
    "        center_y = annotation['bbox'][1] + annotation['bbox'][3]//2\n",
    "        if self.downsample_image_before_crop:\n",
    "            # Most of the images are 640x480-ish, so if we can resize to around 256x256 and then crop we will get more in frame.\n",
    "            img.resize((img.size[0]//2, img.size[1]//2))\n",
    "            mask.resize((mask.size[0]//2, mask.size[1]//2))\n",
    "            center_x = center_x // 2  # Don't forget to move these!\n",
    "            center_y = center_y // 2\n",
    "        # Try and crop around the center.\n",
    "        left = max(0, center_x - self.target_width//2)\n",
    "        top = max(0, center_y - self.target_height//2)\n",
    "        right = left + self.target_width\n",
    "        bottom = top + self.target_height\n",
    "        crop_box = (left, top, right, bottom)\n",
    "        img_crop = img.crop(crop_box)\n",
    "        mask_crop = mask.crop(crop_box)\n",
    "\n",
    "        return img_crop, mask_crop\n",
    "\n",
    "    def get_random_font(self):\n",
    "        \"\"\"We need to lazy load the fonts because the dataset loaders are assumed to run out of thread and ImageFont is not serializable.\"\"\"\n",
    "        if self.font_choices is None:\n",
    "            self.font_choices = list()\n",
    "            for font_filename in iglob(self.font_directory):\n",
    "                for font_size in self.font_sizes:\n",
    "                    self.font_choices.append(ImageFont.truetype(font_filename, font_size))\n",
    "        return random.choice(self.font_choices)\n",
    "\n",
    "    def random_text_image(self):\n",
    "        \"\"\"Generate randomly oriented white text on a black background.  RGB image.  Can be used as a mask.\"\"\"\n",
    "        # TODO: This isn't a pretty thing, but it works.\n",
    "        text = \"\".join(random.choice(string.ascii_letters + string.punctuation + \" \" * 4) for _ in range(self.random_text_length))\n",
    "        text_image = Image.new(\"RGB\", (self.target_width, self.target_height), \"black\")\n",
    "        d = ImageDraw.Draw(text_image)\n",
    "        # d.line(((0, 100), (200, 100)), \"gray\")\n",
    "        # d.line(((100, 0), (100, 200)), \"gray\")\n",
    "        if self.random_text_max_translation > 0:\n",
    "            # Note that this translates before rotating, so our offset might be a little weird.\n",
    "            text_position = (\n",
    "                self.image_center[0] + random.randint(-self.random_text_max_translation, self.random_text_max_translation),\n",
    "                self.image_center[1] + random.randint(-self.random_text_max_translation, self.random_text_max_translation),\n",
    "            )\n",
    "        else:\n",
    "            text_position = self.image_center\n",
    "        d.text(text_position, text, fill=\"white\", anchor=\"mm\", align=\"center\", font=self.get_random_font())\n",
    "        if self.random_text_rotation:\n",
    "            rotation = random.randint(0, 359)\n",
    "        else:\n",
    "            rotation = 0\n",
    "        text_image = text_image.rotate(rotation)\n",
    "        return text_image, text, rotation\n",
    "\n",
    "    def generate_image_mask_pair(self, index):\n",
    "        # We assume we're starting with PIL images for everything AND that they have no text.\n",
    "        img_pil = Image.open(self.textless_images[index]).convert('RGB')\n",
    "\n",
    "        # Randomly mutate the input image.\n",
    "        if random.choice([False, True]):\n",
    "            img_pil = img_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if random.choice([False, True]):\n",
    "            img_pil = img_pil.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        if random.choice([False, True]):\n",
    "            img_pil = img_pil.rotate(random.randint(0, 359))\n",
    "\n",
    "        # If the image is big enough for a random crop, do it.\n",
    "        if img_pil.size[0] > self.target_width and img_pil.size[1] > self.target_height:\n",
    "            left = random.randint(0, img_pil.size[0]-1-self.target_width)\n",
    "            top = random.randint(0, img_pil.size[1] - 1 - self.target_height)\n",
    "            img_pil = img_pil.crop((left, top, left+self.target_width, top+self.target_height))\n",
    "        else:\n",
    "            img_pil = img_pil.resize((self.target_width, self.target_height))\n",
    "\n",
    "        # Generate some random text:\n",
    "        text_image_mask, text, text_rotation = self.random_text_image()\n",
    "        text_image_mask = text_image_mask.convert('L')\n",
    "\n",
    "        # Glorious hack to make a red mask:\n",
    "        # red_channel = img_pil[0].point(lambda i: i < 100 and 255)\n",
    "\n",
    "        # Draw the text image on top of our sample image.\n",
    "        # if (red * 0.299 + green * 0.587 + blue * 0.114) > 186 use  # 000000 else use #ffffff\n",
    "        total_color = [0, 0, 0]\n",
    "        total_pixels = 0\n",
    "        for y in range(self.target_height):\n",
    "            for x in range(self.target_width):\n",
    "                mask = text_image_mask.getpixel((x, y))\n",
    "                if mask > 128:\n",
    "                    px = img_pil.getpixel((x, y))\n",
    "                    total_color[0] += px[0]\n",
    "                    total_color[1] += px[1]\n",
    "                    total_color[2] += px[2]\n",
    "                    total_pixels += 1\n",
    "\n",
    "        # In the off chance we have a completely blank image, save some compute.\n",
    "        if total_pixels > 0:\n",
    "            avg_r = total_color[0]//total_pixels\n",
    "            avg_g = total_color[1]//total_pixels\n",
    "            avg_b = total_color[2]//total_pixels\n",
    "\n",
    "            # Default to light color...\n",
    "            text_color = [random.randint(150, 255), random.randint(150, 255), random.randint(150, 255)]\n",
    "            if (avg_r*0.299 + avg_g*0.587 + avg_b * 0.114) > 125:  # 186 comes from the algorithm but is a little hard to see.\n",
    "                # Unless our image is bright, in which case use dark color.\n",
    "                text_color = [random.randint(0, 75), random.randint(0, 75), random.randint(0, 75)]\n",
    "            # Have to convert text color to a hex string.  :rolleges:\n",
    "            text_color = f\"#{text_color[0]:02X}{text_color[1]:02X}{text_color[2]:02X}\"\n",
    "            # Make a rectangle of this color and paste it in with an image mask.\n",
    "            text_color_block = Image.new(\"RGB\", (self.target_width, self.target_height), color=text_color)\n",
    "            # Maybe add noise to the color block.\n",
    "            if self.text_noise > 0:\n",
    "                pass\n",
    "            img_pil.paste(text_color_block, (0,0), text_image_mask)\n",
    "\n",
    "        # Now dilate the text_image_mask to simulate highlighting a block.\n",
    "        if self.random_text_mask_dilation > 0:\n",
    "            text_image_mask = text_image_mask.filter(ImageFilter.MaxFilter(self.random_text_mask_dilation))\n",
    "\n",
    "        return img_pil, text_image_mask, text, text_rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "CHAR_TO_IDX = {chr(idx):idx for idx in range(ord(' '), ord('~')+1)}\n",
    "CHAR_TO_IDX['\\n'] = len(CHAR_TO_IDX)+1\n",
    "IDX_TO_CHAR = {idx:char for idx, char in CHAR_TO_IDX.items()}  # Could use list.\n",
    "\n",
    "class OCRRNN(nn.Module):\n",
    "    def __init__(self, rnn_latent_size:int, num_layers:int, fov_wh:int = 64):\n",
    "        #self.attention = (0, 0, 1.0)  # xPos 0-1, yPos 0-1, zoom 0-1.  1.0 = whole image.  0.1 = 10% width.\n",
    "        #self.active_image\n",
    "        self.rnn_latent_size = rnn_latent_size\n",
    "        self.image_fov = fov_wh  # Technically not 'field of view' in the rendering sense.  Just width/height.\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size=fov_wh*fov_wh, hidden_size=rnn_latent_size, num_layers=num_layers, batch_first=True)\n",
    "        self.focus_head = nn.Linear(rnn_latent_size, 3)\n",
    "        self.text_head = nn.Linear(rnn_latent_size, len(CHAR_TO_IDX))\n",
    "        \n",
    "    def forward(self, input_state, hidden_state):\n",
    "        \"\"\"\n",
    "        >>> rnn = nn.GRU(10, 20, 2)\n",
    "        >>> input = torch.randn(5, 3, 10)\n",
    "        >>> h0 = torch.randn(2, 3, 20)\n",
    "        >>> output, hn = rnn(input, h0)\n",
    "        \"\"\"\n",
    "        # Convert the PIL image into a tensor, cropped around the appropriate zoom levels.\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return hidden, output, self._new_position_from_output(output), self._char_distribution_from_output(output)\n",
    "    \n",
    "    def _char_distribution_from_output(self, output):\n",
    "        x = self.text_head(output)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def _new_position_from_output(self, output):\n",
    "        return self.focus_head(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
